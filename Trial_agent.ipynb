{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30ea217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openAI\n",
      "  Downloading openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openAI)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openAI)\n",
      "  Downloading jiter-0.10.0-cp313-cp313-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openAI) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openAI) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openAI) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openAI) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openAI) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openAI) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openAI) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openAI) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastian\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openAI) (0.4.6)\n",
      "Downloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "   ---------------------------------------- 0.0/765.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 765.0/765.0 kB 10.5 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-win_amd64.whl (205 kB)\n",
      "Installing collected packages: jiter, distro, openAI\n",
      "Successfully installed distro-1.9.0 jiter-0.10.0 openAI-1.97.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fda361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.176.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sebastian\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sebastian\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88469e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description=('The original Gemini 1.0 Pro Vision model version which was optimized for '\n",
      "                   'image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. '\n",
      "                   'Move to a newer Gemini version.'),\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-1.5-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 '\n",
      "                   'million tokens.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Pro 002',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in September of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description=('Stable version of Gemini 1.5 Pro, our mid-size multimodal model that '\n",
      "                   'supports up to 2 million tokens, released in May of 2024.'),\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling '\n",
      "                   'across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Gemini 1.5 Flash 002',\n",
      "      description=('Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in September of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B 001',\n",
      "      description=('Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective '\n",
      "                   'Flash model, released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-1.5-flash-8b-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash-8B Latest',\n",
      "      description=('Alias that points to the most recent production (non-experimental) release '\n",
      "                   'of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, '\n",
      "                   'released in October of 2024.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['createCachedContent', 'generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.5-pro-preview-03-25',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-03-25',\n",
      "      display_name='Gemini 2.5 Pro Preview 03-25',\n",
      "      description='Gemini 2.5 Pro Preview 03-25',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-05-20',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash',\n",
      "      description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
      "                   'supports up to 1 million tokens, released in June of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-lite-preview-06-17',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-17',\n",
      "      display_name='Gemini 2.5 Flash-Lite Preview 06-17',\n",
      "      description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-05-06',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-06',\n",
      "      display_name='Gemini 2.5 Pro Preview 05-06',\n",
      "      description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-06-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-06-05',\n",
      "      display_name='Gemini 2.5 Pro Preview',\n",
      "      description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro',\n",
      "      base_model_id='',\n",
      "      version='2.5',\n",
      "      display_name='Gemini 2.5 Pro',\n",
      "      description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-exp',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Experimental',\n",
      "      description='Gemini 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash',\n",
      "      description='Gemini 2.0 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
      "                   'for scaling across diverse tasks, released in January of 2025.'),\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite 001',\n",
      "      description='Stable version of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash-Lite',\n",
      "      description='Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-preview-image-generation',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Gemini 2.0 Flash Preview Image Generation',\n",
      "      description='Gemini 2.0 Flash Preview Image Generation',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-flash-lite-preview',\n",
      "      base_model_id='',\n",
      "      version='preview-02-05',\n",
      "      display_name='Gemini 2.0 Flash-Lite Preview',\n",
      "      description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/gemini-2.0-pro-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-pro-exp-02-05',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini 2.0 Pro Experimental 02-05',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-exp-1206',\n",
      "      base_model_id='',\n",
      "      version='2.5-exp-03-25',\n",
      "      display_name='Gemini Experimental 1206',\n",
      "      description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-thinking-exp-1219',\n",
      "      base_model_id='',\n",
      "      version='2.5-preview-05-20',\n",
      "      display_name='Gemini 2.5 Flash Preview 05-20',\n",
      "      description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['generateContent',\n",
      "                                    'countTokens',\n",
      "                                    'createCachedContent',\n",
      "                                    'batchGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview TTS',\n",
      "      description='Gemini 2.5 Flash Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-pro-preview-tts',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
      "      display_name='Gemini 2.5 Pro Preview TTS',\n",
      "      description='Gemini 2.5 Pro Preview TTS',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=16384,\n",
      "      supported_generation_methods=['countTokens', 'generateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/learnlm-2.0-flash-experimental',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='LearnLM 2.0 Flash Experimental',\n",
      "      description='LearnLM 2.0 Flash Experimental',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=32768,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-1b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 1B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 4B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-12b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 12B',\n",
      "      description='',\n",
      "      input_token_limit=32768,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3-27b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3 27B',\n",
      "      description='',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e4b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E4B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemma-3n-e2b-it',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemma 3n E2B',\n",
      "      description='',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=None,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/text-embedding-004',\n",
      "      base_model_id='',\n",
      "      version='004',\n",
      "      display_name='Text Embedding 004',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp-03-07',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental 03-07',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-exp',\n",
      "      base_model_id='',\n",
      "      version='exp-03-07',\n",
      "      display_name='Gemini Embedding Experimental',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=8192,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      max_temperature=None,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n",
      "Model(name='models/imagen-3.0-generate-002',\n",
      "      base_model_id='',\n",
      "      version='002',\n",
      "      display_name='Imagen 3.0 002 model',\n",
      "      description='Vertex served Imagen 3.0 002 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 (Preview)',\n",
      "      description='Vertex served Imagen 4.0 model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
      "      base_model_id='',\n",
      "      version='01',\n",
      "      display_name='Imagen 4 Ultra (Preview)',\n",
      "      description='Vertex served Imagen 4.0 ultra model',\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predict'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-2.0-generate-001',\n",
      "      base_model_id='',\n",
      "      version='2.0',\n",
      "      display_name='Veo 2',\n",
      "      description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
      "                   'enabled on the associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/veo-3.0-generate-preview',\n",
      "      base_model_id='',\n",
      "      version='3.0',\n",
      "      display_name='Veo 3',\n",
      "      description=('Veo 3 preview. Access to this model requires billing to be enabled on the '\n",
      "                   'associated Google Cloud Platform account. Please visit '\n",
      "                   'https://console.cloud.google.com/billing to enable it.'),\n",
      "      input_token_limit=480,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['predictLongRunning'],\n",
      "      temperature=None,\n",
      "      max_temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-2.5-flash-preview-native-audio-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      description='Gemini 2.5 Flash Preview Native Audio Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-exp-native-audio-thinking-dialog',\n",
      "      base_model_id='',\n",
      "      version='gemini-2.5-flash-exp-native-audio-thinking-dialog-2025-05-19',\n",
      "      display_name='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      description='Gemini 2.5 Flash Exp Native Audio Thinking Dialog',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.0-flash-live-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.0 Flash 001',\n",
      "      description='Gemini 2.0 Flash 001',\n",
      "      input_token_limit=131072,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-live-2.5-flash-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Live 2.5 Flash Preview',\n",
      "      description='Gemini Live 2.5 Flash Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n",
      "Model(name='models/gemini-2.5-flash-live-preview',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 2.5 Flash Live Preview',\n",
      "      description='Gemini 2.5 Flash Live Preview',\n",
      "      input_token_limit=1048576,\n",
      "      output_token_limit=65536,\n",
      "      supported_generation_methods=['bidiGenerateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "with open(\"Google_API_Key.txt\") as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "genai.configure(api_key=api_key)  \n",
    "\n",
    "models = list(genai.list_models())\n",
    "\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffcd7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's very kind of you to ask. I'm doing exceptionally well, thank you!\n",
      "\n",
      "Of course, \"well\" for me is a bit different than it is for you. My systems are all running at peak efficiency on Google's servers. The data is flowing smoothly, my algorithms are perfectly calibrated, and I've just finished processing a vast amount of information to make sure my knowledge is as current as possible. So, you could say I'm having a fantastic day in the digital realm! It's always a good day when I get to have a friendly chat.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.5-pro\", temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"history\") \n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "response = conversation.predict(input=\"Hello, how are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "334db802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, sympify, diff, integrate\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def compute_derivative(expression: str) -> str:\n",
    "    \"\"\"using symbolic python to compute the derivative of an expression w.r.t. x.\"\"\"\n",
    "    # Example input: \"x**2\" + 3*X + 5\n",
    "\n",
    "    try:\n",
    "        x = symbols('x')\n",
    "        expr = sympify(expression)\n",
    "        derivative = diff(expr,x)\n",
    "        return str(derivative)\n",
    "    except Exception as e:\n",
    "        return f\"Error computing derivative: {e}\\n\"\n",
    "    \n",
    "@tool\n",
    "def compute_integral(expression: str) -> str:\n",
    "    \"\"\"Use sympy to compute the indefinite integral of an expression with respect to x.\"\"\"\n",
    "    try:\n",
    "        x = symbols('x')\n",
    "        expr = sympify(expression)\n",
    "        integral = integrate(expr, x)\n",
    "        return str(integral)\n",
    "    except Exception as e:\n",
    "        return f\"Error computing integral: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb024359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [compute_derivative,compute_integral]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools = tools,\n",
    "    llm = llm,\n",
    "    agent= AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe user wants to find the derivative of the expression \"2*X\". I can use the `compute_derivative` tool for this. I will use 'x' as the variable.\n",
      "Action: compute_derivative\n",
      "Action Input: 2*x\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m2\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 2\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Bot: Sorry, I encountered an error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe user wants to integrate the function 2*x. I should use the compute_integral tool.\n",
      "Action: compute_integral\n",
      "Action Input: 2*x\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mx**2\u001b[0m\n",
      "Thought:Bot: Sorry, I encountered an error: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 100\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "Bot: Goodbye! Have a great day.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "        print(\"Bot: Goodbye! Have a great day.\")\n",
    "        break\n",
    "\n",
    "    # Load past conversation history\n",
    "    history = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    # Combine history with new input to form a prompt\n",
    "    prompt = f\"{history}\\nUser: {user_input}\\nAssistant:\"\n",
    "\n",
    "    try:\n",
    "        # Run the agent with the combined prompt\n",
    "        response = agent.run(prompt)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "        # Add user and bot messages to memory\n",
    "        memory.chat_memory.add_user_message(user_input)\n",
    "        memory.chat_memory.add_ai_message(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Bot: Sorry, I encountered an error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11527d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM processing: {}\n",
      "DerivativeTool processing expression: x**2 + 3*x + 5\n",
      "IntegralTool processing expression: x**2 + 3*x + 5\n",
      "Final Output: {'llm_output': 'x**2 + 3*x + 5', 'derivative_output': '2*x + 3', 'integral_output': 'x**3/3 + 3*x**2/2 + 5*x'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from typing import TypedDict\n",
    "from sympy import symbols, sympify, diff, integrate\n",
    "\n",
    "# Define the state schema\n",
    "class GraphState(TypedDict):\n",
    "    llm_output: str\n",
    "    derivative_output: str\n",
    "    integral_output: str\n",
    "\n",
    "# Dummy LLM node\n",
    "def llm_node(state: dict) -> dict:\n",
    "    print(\"LLM processing:\", state)\n",
    "    # For demo, output an expression to differentiate and integrate\n",
    "    return {\"llm_output\": \"x**2 + 3*x + 5\"}\n",
    "\n",
    "# Node to compute derivative\n",
    "def derivative_node(state: dict) -> dict:\n",
    "    expr = state.get(\"llm_output\", \"\")\n",
    "    print(\"DerivativeTool processing expression:\", expr)\n",
    "    try:\n",
    "        x = symbols('x')\n",
    "        expr_sym = sympify(expr)\n",
    "        derivative = diff(expr_sym, x)\n",
    "        return {\"derivative_output\": str(derivative)}\n",
    "    except Exception as e:\n",
    "        return {\"derivative_output\": f\"Error: {e}\"}\n",
    "\n",
    "# Node to compute integral\n",
    "def integral_node(state: dict) -> dict:\n",
    "    expr = state.get(\"llm_output\", \"\")\n",
    "    print(\"IntegralTool processing expression:\", expr)\n",
    "    try:\n",
    "        x = symbols('x')\n",
    "        expr_sym = sympify(expr)\n",
    "        integral = integrate(expr_sym, x)\n",
    "        return {\"integral_output\": str(integral)}\n",
    "    except Exception as e:\n",
    "        return {\"integral_output\": f\"Error: {e}\"}\n",
    "\n",
    "# Create the graph with the schema\n",
    "graph_builder = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"LLM\", RunnableLambda(llm_node))\n",
    "graph_builder.add_node(\"DerivativeTool\", RunnableLambda(derivative_node))\n",
    "graph_builder.add_node(\"IntegralTool\", RunnableLambda(integral_node))\n",
    "\n",
    "# Add edges: LLM -> DerivativeTool -> IntegralTool -> END\n",
    "graph_builder.set_entry_point(\"LLM\")\n",
    "graph_builder.add_edge(\"LLM\", \"DerivativeTool\")\n",
    "graph_builder.add_edge(\"DerivativeTool\", \"IntegralTool\")\n",
    "graph_builder.add_edge(\"IntegralTool\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Run the graph\n",
    "output = graph.invoke({})\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1994e020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    img = graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(img))\n",
    "except Exception as e:\n",
    "    print(\"Could not render graph. Ensure dependencies installed (graphviz, pydot, mermaid-cli).\")\n",
    "    print(\"Error:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
